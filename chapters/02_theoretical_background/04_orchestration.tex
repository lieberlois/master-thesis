\section{Containerization and Orchestration}
\label{section:orchestration-scheduling}

    In the era of cloud-native development, containerized workloads are the standard approach for running software -- be it on bare metal, VMs, orchestration tools like K8s or even in the cloud.

    \subsection{Containerization}
    \label{subsection:containers-and-os}
    
    By containerization, the process of bundling applications with all the components they require in order to run on any kind of underlying infrastructure is described. This is once again a good match for IIoT systems since running workloads on a broad variety of infrastructure is a common practice there. The most common tool for this is Docker, which can be seen as an industry standard and is often even used as a synonym for containerization in general. 
    
    For running containers in production, custom operating systems (OS) that are optimized for hosting containerized applications are used. Since containers ship all required tools and libraries necessary to run their respective applications, everything that does not contribute to hosting containers can be seen as overhead, so minimal operating systems are often preferred. Not only does such an OS have a lower resource footprint and is hence better suited for efficient usage of available resources, it also has other benefits. A container-optimized OS for example provides a much smaller attack surface since the number of potential vulnerabilities that an attacker can exploit is drastically reduced by only having the bare minimum of components installed in the first place. Stronger security is also achieved by having locked-down firewalls and strict update mechanisms in place by default. Most operating systems of this type are also configured for automatic initialization so that no on-host setup is required. Common examples for such container-optimized operating systems include ``Flatcar Linux'', ``K3OS'', ``Talox Linux'', ``Fedora CoreOS'' and the meanwhile deprecated ``CoreOS Container Linux'' \cite{google_container_optimized}. The upcoming \autoref{subsection:orchestration-and-scheduling} will discuss, how running and managing containerized workloads at scale can be achieved.
    
    \subsection{Orchestration and Scheduling}
    \label{subsection:orchestration-and-scheduling}

    By using techniques like containerization, the underlying infrastructure poses less of a challenge, even when dealing with different processor architectures, edge devices, bare-metal machines or systems in the cloud. However, this does not solve the issue of orchestration and scheduling of services. Orchestration in such a distributed and dynamic environment poses a significant challenge and is infeasible to perform manually at scale. Tools to manage containers that automate tasks like scaling, rolling upgrades or high availability are essential to ensure consistent service delivery and maintain the overall health and performance of applications in such complex systems, where manual service management is often impossible. It hence doesn't come as a surprise that container virtualization and the orchestration of containers onto a heterogeneous set of delivery targets is a recent trend in the IIoT environment that shows high potential \cite{alamoush_adapting_2022}. Since IIoT systems often come with a variety of environments (i.e.\ edge, fog and cloud) it is desirable to commit to one technology that is able to fulfill the requirements for all of the environments in order to build a uniform system. While tools like ``Hashicorp Nomad'' or ``Docker Swarm'' exist, the current industry-wide consensus focuses mainly on ``Kubernetes'', which this work will hence also focus on. Since a large amount of cloud-native technologies are compatible with or even based on Kubernetes, this is a safe choice for an orchestration tool. Kubernetes can support applications that are distributed over hundreds if not thousands of nodes on a connected network and has the potential to provide effective scheduling and orchestration functionality for resources even across the network edge. While Kubernetes brings the features necessary to ensure scalability and reliability across environments it is also challenging to work with in resource-constrained environments like edge devices (\autoref{section:edge-computing}). These edge devices or microcontrollers are often not powerful enough to run a full-fledged Kubernetes solution upon themselves. To address this issue, custom edge-optimized Kubernetes distributions are one solution. These lightweight variants perform well even in resource-constrained edge environments and solve the main problems that make running Kubernetes on such devices challenging e.g.\ by using overlay networks in favor of virtual private networks. These solutions also have lower memory and CPU requirements so that the available resources can be used for running workloads more efficiently. Distributions like Rancher K3s also reduce the complexity of Kubernetes by bundling all components into a single binary and thus simplify operations of multiple Kubernetes environments. Finally, edge-optimized Kubernetes distributions typically support processor architectures that are common in edge devices like the arm64 architecture  \cite{simpkins_opportunities_2022, weaveworks_gitops_edge_all_clouds, cilic_performance_2023}. An alternative to lightweight Kubernetes is provided by tools like ``KubeEdge'' that extend the scheduling and orchestration functionality to edge devices, without running Kubernetes on them directly. This typically works by having the Kubernetes control plane, which is responsible for tasks like scheduling and orchestration, in a cloud or data center where processing power is available, and only running a small agent that follows the control plane's signals on the actual edge device. The more common approach is running lightweight Kubernetes distributions however, and will also be the approach in this work.

    While Kubernetes acts as a comfortable abstraction over the hardware in use, it remains important for engineers to stay aware of the underlying infrastructure. Not only do containers depend on the physical hardware and thus e.g.\ have to be built for the according processor architecture in use, some functionalities of Kubernetes like replication and redundancy, that usually increase robustness and reliability, also become irrelevant when running a single node Kubernetes instance. Engineers must hence stay close to the systems and have proper observability and monitoring in place to ensure the reliability of the system \cite{simpkins_opportunities_2022}.